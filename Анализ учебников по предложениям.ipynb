{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vakhranev/Pushkina/blob/main/%D0%90%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7%20%D1%83%D1%87%D0%B5%D0%B1%D0%BD%D0%B8%D0%BA%D0%BE%D0%B2%20%D0%BF%D0%BE%20%D0%BF%D1%80%D0%B5%D0%B4%D0%BB%D0%BE%D0%B6%D0%B5%D0%BD%D0%B8%D1%8F%D0%BC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYNZ_uPkx1j4",
        "outputId": "3d158d53-3447-4fde-b5b5-31e95603198e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.2.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install python-docx nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grFDF0__Iust",
        "outputId": "24edebed-f2fc-4012-ed55-96994e6b50b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Сводка совпадений:\n",
            "- old История нашего края. ДНР. 5 класс.docx: 0.18% (7 из 3997 предложений совпадают)\n",
            "- old История нашего края. Запорожская область. 5 класс.docx: 0.05% (2 из 3997 предложений совпадают)\n",
            "- old История нашего края. ЛНР. 5 класс.docx: 0.08% (3 из 3997 предложений совпадают)\n",
            "- old История нашего края. Херсонская область. 5 класс.docx: 0.03% (1 из 3997 предложений совпадают)\n"
          ]
        }
      ],
      "source": [
        "import docx\n",
        "import nltk\n",
        "import os\n",
        "from difflib import SequenceMatcher\n",
        "import csv\n",
        "import re\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "def extract_text_from_docx(path):\n",
        "    doc = docx.Document(path)\n",
        "    full_text = \"\\n\".join(p.text.strip() for p in doc.paragraphs if p.text.strip())\n",
        "    return full_text\n",
        "\n",
        "def clean_and_filter_sentences(text, min_words=3, min_chars=7):\n",
        "    raw_sents = sent_tokenize(text, language='russian')\n",
        "    filtered = []\n",
        "    for s in raw_sents:\n",
        "        s_clean = s.strip()\n",
        "        # Удалим технические маркеры и слишком короткие предложения\n",
        "        s_clean = re.sub(r'^\\d+[.)]?\\s*', '', s_clean)  # убираем \"1.\", \"2)\", \"3. \" и т.п.\n",
        "        if len(s_clean) < min_chars:\n",
        "            continue\n",
        "        if len(s_clean.split()) < min_words:\n",
        "            continue\n",
        "        filtered.append(s_clean)\n",
        "    return filtered\n",
        "\n",
        "def is_similar(a, b, threshold=0.95):\n",
        "    return SequenceMatcher(None, a, b).ratio() >= threshold\n",
        "\n",
        "def compare_sentences(base_sents, other_sents, threshold=0.95):\n",
        "    matches = []\n",
        "    for sent1 in base_sents:\n",
        "        for sent2 in other_sents:\n",
        "            if is_similar(sent1, sent2, threshold):\n",
        "                matches.append((sent1, sent2))\n",
        "                break  # считаем только первое совпадение\n",
        "    return matches\n",
        "\n",
        "def process_files(new_path, old_paths, threshold=0.95):\n",
        "    new_text = extract_text_from_docx(new_path)\n",
        "    new_sents = clean_and_filter_sentences(new_text)\n",
        "\n",
        "    summary = []\n",
        "    for old_path in old_paths:\n",
        "        old_text = extract_text_from_docx(old_path)\n",
        "        old_sents = clean_and_filter_sentences(old_text)\n",
        "\n",
        "        matches = compare_sentences(new_sents, old_sents, threshold=threshold)\n",
        "        percent = len(matches) / len(new_sents) * 100\n",
        "\n",
        "        summary.append({\n",
        "            'file': os.path.basename(old_path),\n",
        "            'matches': len(matches),\n",
        "            'total': len(new_sents),\n",
        "            'percent': round(percent, 2),\n",
        "            'matched_pairs': matches,\n",
        "        })\n",
        "\n",
        "        # save matched pairs\n",
        "        output_file = f\"matches_{os.path.basename(old_path)}.csv\"\n",
        "        with open(output_file, 'w', encoding='utf-8', newline='') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(['Новый учебник', 'Старый учебник'])\n",
        "            writer.writerows(matches)\n",
        "\n",
        "    return summary\n",
        "\n",
        "# === Пример использования ===\n",
        "new_file = 'new История нашего края. Донбасс и Новороссия. 5 класс.docx'\n",
        "old_files = [\n",
        "    'old История нашего края. ДНР. 5 класс.docx',\n",
        "    'old История нашего края. Запорожская область. 5 класс.docx',\n",
        "    'old История нашего края. ЛНР. 5 класс.docx',\n",
        "    'old История нашего края. Херсонская область. 5 класс.docx'\n",
        "]\n",
        "\n",
        "results = process_files(new_file, old_files, threshold=0.95)\n",
        "\n",
        "print(\"Сводка совпадений:\")\n",
        "for r in results:\n",
        "    print(f\"- {r['file']}: {r['percent']}% ({r['matches']} из {r['total']} предложений совпадают)\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmsKvKwvQ1eGwHDnNrhbIR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}